{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment denotion\n",
    "EXP_DESC = {1:\"DMP\",2:\"Sub\",3:\"Nov\",4:\"Random\"}\n",
    "EXP_NUM = 4     #denotes which experiment is on [1,2,3]\n",
    "\n",
    "#Datasets\n",
    "DATASET_LIST=[\"CAPACITIVE\",\"OPTICAL\",\"NIST\"]\n",
    "DATASET=\"CAPACITIVE\"\n",
    "\n",
    "#FMR denotation\n",
    "FMR_DESC = {\"CAPACITIVE\":{1.0:35, 0.1:50, 0.01:65}, \"OPTICAL\":{1.0:18, 0.1:30, 0.01:40}}\n",
    "FMR = 1.0    #denotes which FMR using\n",
    "\n",
    "#Train/Test\n",
    "TT_DESC=[\"train\",\"full\"]\n",
    "TT_SET=\"train\"   #denotes which set evaluating\n",
    "\n",
    "#Generator\n",
    "GENERATOR = None   #set it using variables later\n",
    "\n",
    "#Classifier\n",
    "#CLASS_TYPES=[\"Verifinger\",\"Innovatrics\",\"Bozorth3\",\"MLC\"]\n",
    "CLASS_TYPES=[\"Verifinger\",\"MLC\"]\n",
    "CLASSIFIER=\"Verifinger\"\n",
    "\n",
    "\n",
    "CURRENT_USERS = list(range(720))   #use defaults for now\n",
    "FULL_USER_LIST = list(range(720))\n",
    "NUM_USERS = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get imports\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "from keras.preprocessing.image import img_to_array, ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "import cma\n",
    "sys.path.insert(1, 'PATH/TO/VERIFINGER/SDK/') #insert path to Verifinger SDK\n",
    "import sub_verifinger as sv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"   #define gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR = None\n",
    "\n",
    "#import the variational autoencoder\n",
    "def import_VAE(dataset,vae_size):\n",
    "    return keras.models.load_model(f\"autoencoder_models/print_{dataset}_var_decoder-{vae_size}.h5\")     #change this to the correct path\n",
    "\n",
    "#create a single sample using the generator given an input latent vector\n",
    "def generateSample(x):\n",
    "    xs = np.array(x)\n",
    "    if(len(xs.shape)==1):   #assume 1d\n",
    "        xs = np.expand_dims(xs,axis=0)\n",
    "        \n",
    "    return np.array(GENERATOR(xs,training=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the contents of a txt file to an archive dictionary\n",
    "def importArchiveTxt(fpath):\n",
    "    all_arcs = []\n",
    "    temp_arc = {}\n",
    "    \n",
    "    #open, read, and close file\n",
    "    f = open(fpath, 'r')\n",
    "    entries = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    #add each line as an entry to the archive\n",
    "    for e in entries:\n",
    "        if \"~ ~ ~\" in e:\n",
    "            all_arcs.append(temp_arc)\n",
    "            temp_arc={}\n",
    "            continue\n",
    "            \n",
    "        ep = e.split(\":\")\n",
    "        binnum = ep[0]\n",
    "        x = ep[1]\n",
    "        \n",
    "        temp_arc[binnum]=[float(i) for i in x.split(\",\")]\n",
    "        \n",
    "    return all_arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the user list to a binary value for storage within the archive\n",
    "def userlist2Bin(classUserList,numUsers):\n",
    "    bstr = np.zeros(numUsers)\n",
    "    for u in classUserList:\n",
    "        nu = u-min(FULL_USER_LIST)\n",
    "        if u in FULL_USER_LIST:\n",
    "            bstr[nu] = 1\n",
    "    return \"\".join([str(int(i)) for i in bstr])\n",
    "\n",
    "#convert a binary number to the user list\n",
    "def bin2Userlist(bstr):\n",
    "    ulist=[]\n",
    "    for i,b in enumerate(bstr):\n",
    "        if int(b) == 1:\n",
    "            ulist.append(i+min(FULL_USER_LIST))\n",
    "    return ulist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sub_verifinger as sv  #import verifinger wrapper\n",
    "CLASSIFIER_MODEL=None               #set MLC model to call later\n",
    "\n",
    "#import multi classifier model\n",
    "def setMLC(dataset):\n",
    "    if dataset=='CAPACITIVE':\n",
    "        return keras.models.load_model('multiclass_models/print_multiclassifier_720.h5')     #change this to the correct path\n",
    "    #add other models here\n",
    "    \n",
    "#get user classification from a specific model type\n",
    "#returns an array of ints corresponding to the user id (starting from 0)\n",
    "def getUserPredictions(img,model,dataset,threshold,t_set='full',name='temp'):\n",
    "    global CLASSIFIER_MODEL\n",
    "    #check the input image formatting\n",
    "    img_m=np.array(img)\n",
    "    if len(img_m.shape) == 2:   # assume shape is [#,#]\n",
    "        img_m=np.expand_dims(img_m,axis=2)\n",
    "    if len(img_m.shape) == 3:   # assume shape is [#,#,1]\n",
    "        img_m=np.expand_dims(img_m,axis=0)\n",
    "    \n",
    "    #get predictions straight from the mlc model\n",
    "    if model=='MLC':\n",
    "        thresh_p = threshold/100.0\n",
    "        pred = CLASSIFIER_MODEL.predict(img_m)[0]\n",
    "        users = np.where(pred >= thresh_p)[0]\n",
    "        return users\n",
    "    \n",
    "    #verifinger calls a wrapper to get user results\n",
    "    elif model=='Verifinger':\n",
    "        users = sv.usersMatched(img_m, dataset.lower(),threshold,name,t_set)\n",
    "        return [int(u) for u in users]\n",
    "        \n",
    "    #no other model found\n",
    "    else:\n",
    "        print(f\"No model [ {model} ] found\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the coverage of users by the archive\n",
    "def calcArcCoverage(arc,numUsers):\n",
    "    usersCovered = np.zeros(numUsers)\n",
    "    for c in arc.keys():\n",
    "        b = [int(z) for z in c]   #convert to bitwise array (assume len==numUsers)\n",
    "        \n",
    "        #bitwise operation rewrite\n",
    "        for i in range(numUsers):\n",
    "            usersCovered[i] = (int(usersCovered[i]) | b[i])\n",
    "    \n",
    "    #return percentage of users found\n",
    "    return sum(usersCovered)/numUsers\n",
    "        \n",
    "\n",
    "#generate a report for the experiment based on the coverage and parameters\n",
    "def archiveCoverageReport(coverArr,fname):\n",
    "    #save parameters\n",
    "    rep = {}\n",
    "    rep[\"EXPERIMENT\"] = EXP_DESC[EXP_NUM]\n",
    "    rep[\"DATASET\"] = DATASET\n",
    "    rep[\"FMR\"] = FMR\n",
    "    rep[\"TTYPE\"] = TT_SET\n",
    "    rep[\"CLASSIFIER\"] = CLASSIFIER\n",
    "    \n",
    "    #save full coverage array (sorted from greatest to least)\n",
    "    rep[\"COVERAGE\"] = sorted(coverArr,reverse=True)\n",
    "    \n",
    "    #print to a file\n",
    "    with open(fname, 'a+') as f:\n",
    "        for k,v in rep.items():\n",
    "            f.write(f\"{k}:{v}\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_date = \"\"\n",
    "def runTestCov(archive_path):\n",
    "    global CURRENT_USERS\n",
    "    global FULL_USER_LIST\n",
    "    global NUM_USERS\n",
    "    \n",
    "    # -- DEEP MASTERPRINTS EXPERIMENT -- #\n",
    "    if EXP_NUM == 1:\n",
    "        \n",
    "        #set number of users and initial userset\n",
    "        NUM_USERS = 360\n",
    "    \n",
    "        CURRENT_USERS = list(range(360,720))   \n",
    "        FULL_USER_LIST = list(range(360,720))\n",
    "        \n",
    "        #intialize coverage array\n",
    "        COVERAGE_ARR = []\n",
    "         \n",
    "        #IMPORT THE ARCHIVE LIST WITH Z VECTORS\n",
    "        ARCHIVE_SET = importArchiveTxt(archive_path)\n",
    "        print(len(ARCHIVE_SET))\n",
    "        print(len(ARCHIVE_SET[0]))\n",
    "        \n",
    "        t=0\n",
    "        comb_users = None\n",
    "        for a in ARCHIVE_SET:\n",
    "            print(f\"---  TRIAL #{t} ---\")\n",
    "            \n",
    "            ARCHIVE = a\n",
    "            \n",
    "            for k,z in ARCHIVE.items():   #should only be one\n",
    "                #get user list and binary number\n",
    "                fmr_val = FMR_DESC[DATASET][FMR]\n",
    "                print_name = f\"print_{DATASET}_{fmr_val}-exp_{EXP_NUM}\"\n",
    "                gen_print = generateSample(z)\n",
    "                usersFound = getUserPredictions(gen_print,CLASSIFIER,DATASET,fmr_val,TT_SET,print_name)\n",
    "\n",
    "                user_binstr = userlist2Bin(usersFound,NUM_USERS)\n",
    "                \n",
    "                if not comb_users:       # comb user is user id's found in all prints\n",
    "                    comb_users = int(user_binstr,2)\n",
    "                else:\n",
    "                    comb_users = int(user_binstr,2) | comb_users\n",
    "                \n",
    "                print(f\"users found: {usersFound} | bin: {user_binstr}\")\n",
    "                \n",
    "                fake_arc={user_binstr:z}\n",
    "\n",
    "                #save coverage (single item in archive)\n",
    "                cov = calcArcCoverage(fake_arc,NUM_USERS)\n",
    "                print(f\"Coverage: {cov}\")\n",
    "                COVERAGE_ARR.append(cov)\n",
    "\n",
    "            t+=1\n",
    "            \n",
    "        print(f\"COMBINED USERS: {comb_users}\")\n",
    "        print(f\"COMBINED USERS #: {str(bin(comb_users)).count('1')}\")\n",
    "        \n",
    "        \n",
    "        #export report\n",
    "#         cov_d=\"sp-exp_coverage-out/\"\n",
    "#         if not os.path.exists(cov_d):\n",
    "#             os.mkdir(cov_d)\n",
    "#         cov_report_filename = f\"[SPEXP-coverage]_report_EXP-{EXP_NUM}_{arc_date}.txt\"\n",
    "#         archiveCoverageReport(COVERAGE_ARR,os.path.join(cov_d,cov_report_filename))\n",
    "            \n",
    "            \n",
    "    # -- SUBPRINT EXPERIMENT -- #\n",
    "    elif EXP_NUM == 2:\n",
    "        #intialize coverage array\n",
    "        COVERAGE_ARR = []\n",
    "        \n",
    "        #IMPORT THE ARCHIVE LIST OF Z VECTORS\n",
    "        ARCHIVE_SET = importArchiveTxt(archive_path)\n",
    "        print(f\"TRIALS: {len(ARCHIVE_SET)}\")\n",
    "        print(f\"Members: {len(ARCHIVE_SET[0])}\")\n",
    "        \n",
    "        t = 0\n",
    "        for a in ARCHIVE_SET:\n",
    "        \n",
    "            print(f\"---------     TRIAL #{t+1}    ----------\")\n",
    "            \n",
    "            ARCHIVE = a\n",
    "\n",
    "            #set number of users and initial userset\n",
    "            NUM_USERS = 360    \n",
    "            CURRENT_USERS = list(range(360,720))   \n",
    "            FULL_USER_LIST = list(range(360,720))     \n",
    "            \n",
    "           \n",
    "            NEW_ARCHIVE = {}\n",
    "            t2=0\n",
    "            for k,z in ARCHIVE.items():\n",
    "                print(f\"*** Archive iteration {t2+1} / {len(ARCHIVE)} ***\")\n",
    "\n",
    "                #get user list and binary number\n",
    "                fmr_val = FMR_DESC[DATASET][FMR]\n",
    "                print_name = f\"print_{DATASET}_{fmr_val}-exp_{EXP_NUM}\"\n",
    "                gen_print = generateSample(z)\n",
    "                usersFound = getUserPredictions(gen_print,CLASSIFIER,DATASET,fmr_val,TT_SET,print_name)\n",
    "\n",
    "                #SUBPRINTS - REMOVE USERS FOUND\n",
    "                for u in usersFound:\n",
    "                    if u in CURRENT_USERS:\n",
    "                        CURRENT_USERS.remove(u)\n",
    "\n",
    "                #add to lv and classification bin string to archive\n",
    "                user_binstr = userlist2Bin(usersFound,NUM_USERS)\n",
    "                NEW_ARCHIVE[user_binstr] = z\n",
    "                t2+=1\n",
    "            t+=1\n",
    "\n",
    "            #save coverage (single item in archive)\n",
    "            cov = calcArcCoverage(NEW_ARCHIVE,NUM_USERS)\n",
    "            print(f\"Coverage: {cov}\")\n",
    "            COVERAGE_ARR.append(cov)\n",
    "            \n",
    "        \n",
    "        #export report\n",
    "#         cov_d=\"sp-exp_coverage-out/\"\n",
    "#         if not os.path.exists(cov_d):\n",
    "#             os.mkdir(cov_d)\n",
    "#         cov_report_filename = f\"[SPEXP-coverage]_report_EXP-{EXP_NUM}_{arc_date}.txt\"\n",
    "#         archiveCoverageReport(COVERAGE_ARR,os.path.join(cov_d,cov_report_filename))\n",
    "        \n",
    "        \n",
    "        \n",
    "    # -- NOVELTY EXPERIMENT -- #\n",
    "    elif EXP_NUM == 3:\n",
    "        #intialize coverage array\n",
    "        COVERAGE_ARR = []\n",
    "        \n",
    "        #IMPORT THE ARCHIVE LIST OF Z VECTORS\n",
    "        ARCHIVE_SET = importArchiveTxt(archive_path)\n",
    "        print(f\"TRIALS: {len(ARCHIVE_SET)}\")\n",
    "        print(f\"Members: {len(ARCHIVE_SET[0])}\")\n",
    "        \n",
    "        t = 0\n",
    "        for a in ARCHIVE_SET:\n",
    "        \n",
    "            print(f\"---------     TRIAL #{t+1}    ----------\")\n",
    "            \n",
    "            ARCHIVE = a\n",
    "\n",
    "            #set number of users and initial userset\n",
    "            NUM_USERS = 360    \n",
    "            CURRENT_USERS = list(range(360,720))   \n",
    "            FULL_USER_LIST = list(range(360,720))    \n",
    "            \n",
    "            \n",
    "            NEW_ARCHIVE = {}\n",
    "            t2=0\n",
    "            for k,z in ARCHIVE.items():\n",
    "                print(f\"*** Archive iteration {t2+1} / {len(ARCHIVE)} ***\")\n",
    "\n",
    "                #get user list and binary number\n",
    "                fmr_val = FMR_DESC[DATASET][FMR]\n",
    "                print_name = f\"print_{DATASET}_{fmr_val}-exp_{EXP_NUM}\"\n",
    "                gen_print = generateSample(z)\n",
    "                usersFound = getUserPredictions(gen_print,CLASSIFIER,DATASET,fmr_val,TT_SET,print_name)\n",
    "\n",
    "                #add to lv and classification bin string to archive\n",
    "                user_binstr = userlist2Bin(usersFound,NUM_USERS)\n",
    "                NEW_ARCHIVE[user_binstr] = z\n",
    "                t2+=1\n",
    "            t+=1\n",
    "\n",
    "            #save coverage (single item in archive)\n",
    "            cov = calcArcCoverage(NEW_ARCHIVE,NUM_USERS)\n",
    "            print(f\"Coverage: {cov}\")\n",
    "            COVERAGE_ARR.append(cov)\n",
    "        \n",
    "        #export report\n",
    "        # cov_d=\"sp-exp_coverage-out/\"\n",
    "        # if not os.path.exists(cov_d):\n",
    "        #     os.mkdir(cov_d)\n",
    "        # cov_report_filename = f\"[SPEXP-coverage]_report_EXP-{EXP_NUM}_{arc_date}.txt\"\n",
    "        # archiveCoverageReport(COVERAGE_ARR,os.path.join(cov_d,cov_report_filename))\n",
    "        \n",
    "        \n",
    "    # -- RANDOM ARCHIVE EXPERIMENT -- #\n",
    "    elif EXP_NUM == 4:\n",
    "        #intialize coverage array\n",
    "        COVERAGE_ARR = []\n",
    "        \n",
    "        #IMPORT THE ARCHIVE LIST OF Z VECTORS\n",
    "        ARCHIVE_SET = importArchiveTxt(archive_path)\n",
    "        print(f\"TRIALS: {len(ARCHIVE_SET)}\")\n",
    "        print(f\"Members: {len(ARCHIVE_SET[0])}\")\n",
    "        \n",
    "        t = 0\n",
    "        for a in ARCHIVE_SET:\n",
    "        \n",
    "            print(f\"---------     TRIAL #{t+1}    ----------\")\n",
    "            \n",
    "            ARCHIVE = a\n",
    "\n",
    "            #set number of users and initial userset\n",
    "            NUM_USERS = 360  \n",
    "            CURRENT_USERS = list(range(360,720))   \n",
    "            FULL_USER_LIST = list(range(360,720))    \n",
    "            \n",
    "            \n",
    "            NEW_ARCHIVE = {}\n",
    "            t2=0\n",
    "            for k,z in ARCHIVE.items():\n",
    "                print(f\"*** Archive iteration {t2+1} / {len(ARCHIVE)} ***\")\n",
    "\n",
    "                #get user list and binary number\n",
    "                fmr_val = FMR_DESC[DATASET][FMR]\n",
    "                print_name = f\"print_{DATASET}_{fmr_val}-exp_{EXP_NUM}\"\n",
    "                gen_print = generateSample(z)\n",
    "                usersFound = getUserPredictions(gen_print,CLASSIFIER,DATASET,fmr_val,TT_SET,print_name)\n",
    "\n",
    "                #add to lv and classification bin string to archive\n",
    "                user_binstr = userlist2Bin(usersFound,NUM_USERS)\n",
    "                NEW_ARCHIVE[user_binstr] = z\n",
    "                t2+=1\n",
    "            t+=1\n",
    "\n",
    "            #save coverage (single item in archive)\n",
    "            cov = calcArcCoverage(NEW_ARCHIVE,NUM_USERS)\n",
    "            print(f\"Coverage: {cov}\")\n",
    "            COVERAGE_ARR.append(cov)\n",
    "        \n",
    "        # #export report\n",
    "        # cov_d=\"sp-exp_coverage-out/\"\n",
    "        # if not os.path.exists(cov_d):\n",
    "        #     os.mkdir(cov_d)\n",
    "        # cov_report_filename = f\"[SPEXP-coverage]_report_EXP-{EXP_NUM}_{arc_date}.txt\"\n",
    "        # archiveCoverageReport(COVERAGE_ARR,os.path.join(cov_d,cov_report_filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the archive file by a string match\n",
    "def getMatchArchiveFile(files, mstr):\n",
    "    for f in files:\n",
    "        if mstr in f:\n",
    "            return f\n",
    "    return None   #no match found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment denotion\n",
    "#EXPs = [1,2,3,4]\n",
    "EXPs = [1]\n",
    "EXP_NUM = 1     #denotes which experiment is on [1,2,3,4]\n",
    "\n",
    "#Datasets\n",
    "DATASETs = [\"CAPACITIVE\"]\n",
    "DATASET=\"CAPACITIVE\"\n",
    "\n",
    "#FMR denotation\n",
    "FMRs = [1.0, 0.1, 0.01]\n",
    "FMR = 1.0    #denotes which FMR using\n",
    "\n",
    "#TEST ONLY\n",
    "TTs = [\"test\"]\n",
    "TT_SET=\"test\"   #denotes which set evaluating\n",
    "\n",
    "#Classifier\n",
    "CLASSs=[\"Verifinger\"]\n",
    "CLASSIFIER=\"Verifinger\"\n",
    "\n",
    "arc_date = \"26-Jan-2022\"\n",
    "\n",
    "#do the thing\n",
    "for c in CLASSs:\n",
    "    for d in DATASETs:\n",
    "        if c == \"MLC\":\n",
    "            CLASSIFIER_MODEL = setMLC(d)\n",
    "        for t in TTs:\n",
    "            for e in EXPs:\n",
    "                for f in FMRs:\n",
    "                    \n",
    "                    #set parameters\n",
    "                    EXP_NUM = e\n",
    "                    FMR = f\n",
    "                    TT_SET = t\n",
    "                    DATASET = d\n",
    "                    CLASSIFIER = c\n",
    "                    \n",
    "                    print(f\"-- EXPERIMENT: {EXP_NUM}-- \")\n",
    "                    print(f\"FMR: {FMR}\")\n",
    "                    print(f\"T SET: {TT_SET}\")\n",
    "                    print(f\"DATASET: {DATASET}\")\n",
    "                    print(f\"CLASSIFIER: {CLASSIFIER}\")\n",
    "                    print(\"\")\n",
    "                    \n",
    "                    #set archive path\n",
    "                    arc_dir_path = f\"sp-exp_archive-out/[EXP {EXP_NUM}] ({arc_date})\"\n",
    "                    arc_files = [os.path.join(arc_dir_path, f) for f in os.listdir(arc_dir_path) if os.path.isfile(os.path.join(arc_dir_path, f))]\n",
    "                    arc_fmr_file = getMatchArchiveFile(arc_files,f\"fmr-{FMR}\")\n",
    "                    \n",
    "                    #set generator\n",
    "                    GENERATOR = import_VAE(DATASET,32)\n",
    "                    \n",
    "                    #run it!\n",
    "                    runTestCov(arc_fmr_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepPrint2",
   "language": "python",
   "name": "deepprint2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
