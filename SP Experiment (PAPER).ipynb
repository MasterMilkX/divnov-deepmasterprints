{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milk's Big Subprint Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment denotion\n",
    "EXP_DESC = {1:\"DMP\",2:\"Sub\",3:\"Nov\"}\n",
    "EXP_NUM = 1     #denotes which experiment is on [1,2,3]\n",
    "\n",
    "#Datasets\n",
    "DATASET_LIST=[\"CAPACITIVE\",\"OPTICAL\",\"NIST\"]\n",
    "DATASET=\"CAPACITIVE\"\n",
    "\n",
    "#FMR denotation\n",
    "FMR_DESC = {\"CAPACITIVE\":{1.0:35, 0.1:50, 0.01:65}, \"OPTICAL\":{1.0:18, 0.1:30, 0.01:40}}\n",
    "FMR = 1.0    #denotes which FMR using\n",
    "\n",
    "#Train/Test\n",
    "TT_DESC=[\"train\",\"test\",\"full\"]\n",
    "TT_SET=\"train\"   #denotes which set evaluating\n",
    "\n",
    "#Generator\n",
    "GENERATOR = None   #set it using variables later\n",
    "\n",
    "#Classifier\n",
    "CLASS_TYPES=[\"Verifinger\",\"MLC\"]\n",
    "CLASSIFIER=\"Verifinger\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMA_ITER = 59      #maximum number of iterations to run for CMA-ES objective evolver (X)\n",
    "TRIALS = 10         #Number of experiment trials (DMP) (Y)\n",
    "ARC_ITER = 10     #Maximum size of the archive (Nov and Sub) (Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get imports\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "from keras.preprocessing.image import img_to_array, ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "import cma\n",
    "sys.path.insert(1, 'PATH/TO/VERIFINGER/SDK/') #insert path to Verifinger SDK\n",
    "import sub_verifinger as sv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"   #define gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datasets\n",
    "#define datatsets\n",
    "def load_capacitive_data(dirname = \"../<path to capacity fingerprint dataset>\"):\n",
    "    imgs_names = os.listdir(dirname)\n",
    "    \n",
    "    #Prep Images\n",
    "    X = []\n",
    "    Y = []\n",
    "    with tqdm(total=len(imgs_names)) as pbar:\n",
    "        for i in imgs_names:\n",
    "            if(i.endswith('.BMP')):\n",
    "                im = Image.open(dirname + i)\n",
    "                im = im.crop((8, 8, 136, 136))\n",
    "                im = im.convert('L')\n",
    "                im = img_to_array(im)\n",
    "                im = (255 - im) / 255.0    #normalize [0,1]\n",
    "                #im = (127.5-im) / 127.5   #normalize [-1,1]\n",
    "\n",
    "\n",
    "                Y.append(i)\n",
    "                X.append(im)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    X_train = np.array(X)\n",
    "    Y_train = np.array(Y)\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "def load_optical_data(dirname = \"<path to optical fingerprint dataset>\"):\n",
    "    imgs_names = os.listdir(dirname)\n",
    "    \n",
    "    #Prep Images\n",
    "    X = []\n",
    "    Y = []\n",
    "    with tqdm(total=len(imgs_names)) as pbar:\n",
    "        for i in imgs_names:\n",
    "            if(i.endswith('.tif')):\n",
    "                im = Image.open(dirname + i)\n",
    "                im = im.convert('L')\n",
    "                im = img_to_array(im)\n",
    "                im = (255 - im) / 255.0    #normalize [0,1]\n",
    "                #im = (127.5-im) / 127.5   #normalize [-1,1]\n",
    "\n",
    "                Y.append(i)\n",
    "                X.append(im)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    X_train = np.array(X)\n",
    "    Y_train = np.array(Y)\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "def load_nist_data(s = 256, dirname = \"<path to nist fingerprint dataset>\", randcrop=True, rcs=128):\n",
    "    imgs_names = os.listdir(dirname)\n",
    "    \n",
    "    #Prep Images\n",
    "    X = []\n",
    "    Y = []\n",
    "    with tqdm(total=len(imgs_names)) as pbar:\n",
    "        for i in imgs_names:\n",
    "            im = Image.open(dirname + i)\n",
    "            if(im.size[0] >= s and im.size[1] > s):\n",
    "                Y.append(i)\n",
    "                xi = resize(im,s)\n",
    "                if randcrop:\n",
    "                    xi = random_crop(np.expand_dims(xi,axis=0),rcs)[0]\n",
    "                X.append(xi)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    X_train = np.array(X)\n",
    "    Y_train = np.array(Y)\n",
    "    \n",
    "    return X_train, Y_train\n",
    "\n",
    "def random_crop(img,crop_size):\n",
    "    x, y = (np.random.randint(0, img.shape[1] - crop_size) for i in range(2))\n",
    "    return img[:, x:x + crop_size, y:y + crop_size]\n",
    "\n",
    "#resize\n",
    "def resize(img, s):\n",
    "    if(img.size[0] > img.size[1]):\n",
    "        h = s\n",
    "        ratio = h/float(img.size[1])\n",
    "        w = int(img.size[0]*ratio)\n",
    "        img = img.resize((w,h), Image.ANTIALIAS)\n",
    "        \n",
    "        border = int((w - s)/2)\n",
    "        img = img.crop((border, 0, border + s, h))\n",
    "    else:\n",
    "        w = s\n",
    "        ratio = w/float(img.size[0])\n",
    "        h = int(img.size[1]*ratio)\n",
    "        img = img.resize((w,h), Image.ANTIALIAS)\n",
    "        \n",
    "        border = int((h - s)/2)\n",
    "        img = img.crop((0, border, w, border + s))\n",
    "        \n",
    "    img = img.convert('L')    \n",
    "    #img = preK.img_to_array(img, 'tf')\n",
    "    img = img_to_array(img)\n",
    "    img = (255 - img) / 255.0    #normalize [0,1]\n",
    "    #img = (127.5-img) / 127.5   #normalize [-1,1]\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets a X,y fingerprint dataset and print dictionary with users as keys and array of prints as values\n",
    "def importDataset():\n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    y_clean=[]\n",
    "    \n",
    "    if DATASET == \"CAPACITIVE\":\n",
    "        X_train, y_train = load_capacitive_data()\n",
    "        y_clean = list(map(lambda x: int(x.split(\"_\")[1])-1,y_train))\n",
    "    elif DATASET == \"OPTICAL\":\n",
    "        X_train, y_train = load_optical_data()\n",
    "        y_clean = list(map(lambda x: int(x.split(\"_\")[0]),y_train))\n",
    "    elif DATASET == \"NIST\":\n",
    "        X_train, y_train = load_nist_data()\n",
    "        y_clean = y_train\n",
    "    \n",
    "    print_dict = {}\n",
    "    for i in range(len(X_train)):\n",
    "        if y_clean[i] not in print_dict:\n",
    "            print_dict[y_clean[i]] = []\n",
    "        print_dict[y_clean[i]].append(X_train[i])\n",
    "        \n",
    "    return X_train, y_clean, print_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR = None\n",
    "\n",
    "#import the variational autoencoder\n",
    "def import_VAE(dataset,vae_size):\n",
    "    return keras.models.load_model(f\"autoencoder_models/print_{dataset}_var_decoder-{vae_size}.h5\")\n",
    "\n",
    "#import the wasserstein-gan\n",
    "def import_WGAN(dataset,epoch_num):\n",
    "    #return keras.models.load_model(f\"gan_models/wgan_{dataset}_{epoch_num}_gen.h5\")\n",
    "    return import_VAE(dataset, epoch_num)    #use VAE for now (epoch_num == vae_size)\n",
    "\n",
    "#create a single sample using the generator given an input latent vector\n",
    "def generateSample(x):\n",
    "    xs = np.array(x)\n",
    "    if(len(xs.shape)==1):   #assume 1d\n",
    "        xs = np.expand_dims(xs,axis=0)\n",
    "        \n",
    "    return np.array(GENERATOR(xs,training=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sub_verifinger as sv  #import verifinger wrapper\n",
    "MLC_MODEL=None               #set MLC model to call later\n",
    "\n",
    "#import Multi Label Classifier model\n",
    "def setMLC(dataset):\n",
    "    if dataset=='CAPACITIVE':\n",
    "        return keras.models.load_model('multiclass_models/print_multiclassifier_720.h5')\n",
    "    #add other models here\n",
    "    \n",
    "#get user classification from a specific model type\n",
    "#returns an array of ints corresponding to the user id (starting from 0)\n",
    "def getUserPredictions(img,model,dataset,threshold,t_set='full',name='temp'):\n",
    "    #check the input image formatting\n",
    "    img_m=np.array(img)\n",
    "    if len(img_m.shape) == 2:   # assume shape is [#,#]\n",
    "        img_m=np.expand_dims(img_m,axis=2)\n",
    "    if len(img_m.shape) == 3:   # assume shape is [#,#,1]\n",
    "        img_m=np.expand_dims(img_m,axis=0)\n",
    "    \n",
    "    #get predictions straight from the mlc model\n",
    "    if model=='MLC':\n",
    "        thresh_p = threshold/100.0\n",
    "        pred = MLC_MODEL.predict(img_m)[0]\n",
    "        users = np.where(pred >= thresh_p)[0]\n",
    "        return users\n",
    "    \n",
    "    #verifinger calls a wrapper to get user results\n",
    "    elif model=='Verifinger':\n",
    "        users = sv.usersMatched(img_m, dataset.lower(),threshold,name,t_set)\n",
    "        return [int(u) for u in users]\n",
    "        \n",
    "    #no other model found\n",
    "    else:\n",
    "        print(f\"No model [ {model} ] found\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMA Fitness functions\n",
    "\n",
    "the part that actually runs the CMA-ES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_USERS = list(range(720))   #use defaults for now\n",
    "NUM_USERS = 720\n",
    "\n",
    "NOVELTY_ARCHIVE = []\n",
    "#get novelty score for a vector based on novelty set\n",
    "def novelScore(v,novSet):\n",
    "    #get raw distance from origin \n",
    "    if len(novSet) == 0:\n",
    "        tv = np.zeros(len(v))\n",
    "        return np.linalg.norm(np.array(tv)-np.array(v))\n",
    "    \n",
    "    #get distance to closest vector\n",
    "    else:\n",
    "        d = []\n",
    "        for nv in novSet:\n",
    "            d.append(np.linalg.norm(np.array(nv)-np.array(v)))\n",
    "        return min(d)\n",
    "        \n",
    "\n",
    "#remember, decrease fitness value for cma\n",
    "# FMR is set\n",
    "def fitness(x):\n",
    "    gen_print = generateSample(x)\n",
    "    #plt.imshow(gen_print,cmap='binary')\n",
    "    \n",
    "    fmr_val = FMR_DESC[DATASET][FMR]\n",
    "    print_name = f\"print_{DATASET}_{fmr_val}-exp_{EXP_NUM}\"\n",
    "    usersFound = getUserPredictions(gen_print,CLASSIFIER,DATASET,fmr_val,TT_SET,print_name)\n",
    "    \n",
    "    #normal comparison\n",
    "    if(EXP_NUM in [1,2]):\n",
    "        rem_usersFound = 0\n",
    "        for u in usersFound:\n",
    "            if u in CURRENT_USERS:       #reward for finding new users\n",
    "                rem_usersFound += 1\n",
    "\n",
    "        #return number of users found \n",
    "        # (CMA) decrease fitness value for cma\n",
    "        f = 1.0-(rem_usersFound/len(CURRENT_USERS))\n",
    "        return f\n",
    "    #novelty comparison\n",
    "    else:\n",
    "        zv = np.zeros(NUM_USERS)\n",
    "        for u in usersFound:\n",
    "            zv[u] = 1\n",
    "        return 1.0/novelScore(zv,NOVELTY_ARCHIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the user list to a binary value for storage within the archive\n",
    "def userlist2Bin(classUserList,numUsers):\n",
    "    bstr = np.zeros(numUsers)\n",
    "    for u in classUserList:\n",
    "        bstr[u] = 1\n",
    "    return \"\".join([str(int(i)) for i in bstr])\n",
    "\n",
    "#convert a binary number to the user list\n",
    "def bin2Userlist(bstr):\n",
    "    ulist=[]\n",
    "    for i,b in enumerate(bstr):\n",
    "        if int(b) == 1:\n",
    "            ulist.append(i)\n",
    "    return ulist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create default path and file output path for the archive based on parameters\n",
    "def makeDefaultFilename(i=''):\n",
    "    path = \"sp-exp_archive-out\"\n",
    "    \n",
    "    #check if directory exists - create it if not\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "    #default filename -> experiment #, ttype, fmr, dataset, classifier, [CMA_ITER,TRIALS,ARC_ITER]\n",
    "    extra = f\"__{i}\" if i != '' else ''\n",
    "    fname=f\"[SPEXP-ARCHIVE]_expnum-{EXP_NUM}_ttype-{TT_SET}_fmr-{FMR}_dataset-{DATASET}_classifier-{CLASSIFIER}__[{CMA_ITER}_{TRIALS}_{ARC_ITER}]{extra}.txt\"\n",
    "\n",
    "    return path, fname\n",
    "\n",
    "\n",
    "#export the entire archive text file\n",
    "def exportFullArchiveTxt(arc,path,fname):\n",
    "    #exports each entry in the following form:\n",
    "    #binary#:x\n",
    "    with open(os.path.join(path,fname),'w+') as f:\n",
    "        for k,v in arc.items():\n",
    "            f.write(f\"{k}:{','.join([str(r) for r in v])}\\n\")  \n",
    "    \n",
    "    return fname\n",
    "\n",
    "#add single entry to archive text file\n",
    "def addArchiveTxt(entry,fpath):\n",
    "     with open(fpath,'w+') as f:\n",
    "        f.write(f\"{entry[0]}:{','.join([str(r) for r in entry[1]])}\\n\")\n",
    "       \n",
    "    \n",
    "#add new line for archive output to differentiate between trials\n",
    "def newTrialArchiveTxt(fpath):\n",
    "    with open(fpath,'w+') as f:\n",
    "        f.write(\"~ ~ ~\")\n",
    "    \n",
    "#import the contents of a txt file to an archive dictionary\n",
    "def importArchiveTxt(fpath):\n",
    "    all_arcs = []\n",
    "    temp_arc = {}\n",
    "    \n",
    "    #open, read, and close file\n",
    "    f = open(fpath, 'r')\n",
    "    entries = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    #add each line as an entry to the archive\n",
    "    for e in entries:\n",
    "        if e==\"~ ~ ~\":\n",
    "            all_arcs.append(temp_arc)\n",
    "            temp_arc={}\n",
    "            continue\n",
    "            \n",
    "        ep = e.split(\":\")\n",
    "        binnum = ep[0]\n",
    "        x = ep[1]\n",
    "        \n",
    "        temp_arc[binnum]=x\n",
    "        \n",
    "    return all_arcs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the coverage of users by the archive\n",
    "def calcArcCoverage(arc,numUsers):\n",
    "    usersCovered = np.zeros(numUsers)\n",
    "    for c in arc.keys():\n",
    "        b = [int(z) for z in c]   #convert to bitwise array (assume len==numUsers)\n",
    "        \n",
    "        #bitwise operation rewrite\n",
    "        for i in range(numUsers):\n",
    "            usersCovered[i] = (int(usersCovered[i]) | b[i])\n",
    "    \n",
    "    #return percentage of users found\n",
    "    return sum(usersCovered)/numUsers\n",
    "        \n",
    "\n",
    "#generate a report for the experiment based on the coverage and parameters\n",
    "def archiveCoverageReport(coverArr,fname):\n",
    "    #save parameters\n",
    "    rep = {}\n",
    "    rep[\"EXPERIMENT\"] = EXP_DESC[EXP_NUM]\n",
    "    rep[\"DATASET\"] = DATASET\n",
    "    rep[\"FMR\"] = FMR\n",
    "    rep[\"TTYPE\"] = TT_SET\n",
    "    rep[\"CLASSIFIER\"] = CLASSIFIER\n",
    "    \n",
    "    #save full coverage array (sorted from greatest to least)\n",
    "    rep[\"COVERAGE\"] = sorted(coverArr,reverse=True)\n",
    "    \n",
    "    #print to a file\n",
    "    with open(fname, 'a+') as f:\n",
    "        for k,v in rep.items():\n",
    "            f.write(f\"{k}:{v}\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually build the archive based on the parameters specified\n",
    "def runExp():\n",
    "    global CURRENT_USERS\n",
    "    global NUM_USERS\n",
    "    global NOVELTY_ARCHIVE\n",
    "    \n",
    "    # -- DEEP MASTERPRINTS EXPERIMENT -- #\n",
    "    if EXP_NUM == 1:\n",
    "        \n",
    "        #set number of users and initial userset\n",
    "        if DATASET == \"CAPACITIVE\":\n",
    "            NUM_USERS = 720\n",
    "        else:\n",
    "            NUM_USERS = 720    #change to something else later\n",
    "        CURRENT_USERS = list(range(NUM_USERS))    \n",
    "        \n",
    "        #intialize coverage array\n",
    "        COVERAGE_ARR = []\n",
    "        \n",
    "        #delete old file if it exists\n",
    "        p,f = makeDefaultFilename()\n",
    "        if(os.path.exists(os.path.join(p,f))):\n",
    "            os.remove(os.path.join(o,f))\n",
    "        \n",
    "        for t in range(TRIALS):\n",
    "            print(f\"---  TRIAL #{t} ---\")\n",
    "            #run cma-es\n",
    "            es = cma.CMAEvolutionStrategy(np.random.normal(0,1,100).tolist(), 1, {'maxiter':CMA_ITER})  \n",
    "            es.optimize(fitness)   #fitness function is defined above\n",
    "            z = es.result[0]    # save the best solution\n",
    "            \n",
    "            #get user list and binary number\n",
    "            fmr_val = FMR_DESC[DATASET][FMR]\n",
    "            print_name = f\"print_{DATASET}_{fmr_val}-exp_{EXP_NUM}\"\n",
    "            gen_print = generateSample(z)\n",
    "            usersFound = getUserPredictions(gen_print,CLASSIFIER,DATASET,fmr_val,TT_SET,print_name)\n",
    "            \n",
    "            user_binstr = userlist2Bin(usersFound,NUM_USERS)\n",
    "            fake_arc={user_binstr:z}\n",
    "            \n",
    "            #export to file\n",
    "            addArchiveTxt(ARCHIVE,os.path.join(p,f))\n",
    "             \n",
    "            #save coverage (single item in archive)\n",
    "            cov = calcArcCoverage(fake_arc,NUM_USERS)\n",
    "            COVERAGE_ARR.append(cov)\n",
    "        \n",
    "            newTrialArchiveTxt(os.path.join(p,f))\n",
    "        \n",
    "        #export report\n",
    "        cov_d=\"sp-exp_coverage-out/\"\n",
    "        if not os.path.exists(cov_d):\n",
    "            os.mkdir(cov_d)\n",
    "        cov_report_filename = f\"[SPEXP-coverage]_report_EXP-{EXP_NUM}.txt\"\n",
    "        archiveCoverageReport(COVERAGE_ARR,os.path.join(cov_d,cov_report_filename))\n",
    "            \n",
    "            \n",
    "    # -- SUBPRINT EXPERIMENT -- # .  diversity master prints\n",
    "    elif EXP_NUM == 2:\n",
    "        #intialize coverage array\n",
    "        COVERAGE_ARR = []\n",
    "        \n",
    "        for t in range(TRIALS):\n",
    "            print(f\"---------     TRIAL #{t+1}    ----------\")\n",
    "            \n",
    "            ARCHIVE = {}  #initialize empty archive\n",
    "            NOVELTY_ARCHIVE = []  #initialize empty novelty user archive\n",
    "\n",
    "            #set number of users and initial userset\n",
    "            if DATASET == \"CAPACITIVE\":\n",
    "                NUM_USERS = 720\n",
    "            else:\n",
    "                NUM_USERS = 720    #change to something else later\n",
    "            CURRENT_USERS = list(range(NUM_USERS))    \n",
    "\n",
    "            for AI in range(ARC_ITER):\n",
    "                print(f\"*** Archive iteration {AI+1} / {ARC_ITER} -- USERS: [{len(CURRENT_USERS)} / {NUM_USERS}] ***\")\n",
    "                #run cma-es\n",
    "                es = cma.CMAEvolutionStrategy(np.random.normal(0,1,100).tolist(), 1, {'maxiter':CMA_ITER})\n",
    "                es.optimize(fitness)\n",
    "                z = es.result[0]\n",
    "\n",
    "                #get user list and binary number\n",
    "                fmr_val = FMR_DESC[DATASET][FMR]\n",
    "                print_name = f\"print_{DATASET}_{fmr_val}-exp_{EXP_NUM}\"\n",
    "                gen_print = generateSample(z)\n",
    "                usersFound = getUserPredictions(gen_print,CLASSIFIER,DATASET,fmr_val,TT_SET,print_name)\n",
    "\n",
    "                #SUBPRINTS - REMOVE USERS FOUND\n",
    "                for u in usersFound:\n",
    "                    if u in CURRENT_USERS:\n",
    "                        CURRENT_USERS.remove(u)\n",
    "\n",
    "                #add to lv and classification bin string to archive\n",
    "                user_binstr = userlist2Bin(usersFound,NUM_USERS)\n",
    "                ARCHIVE[user_binstr] = z\n",
    "\n",
    "                #export entry to file\n",
    "                p,f = makeDefaultFilename(\"\")\n",
    "                addArchiveTxt(ARCHIVE,os.path.join(p,f))\n",
    "\n",
    "            #save coverage (single item in archive)\n",
    "            cov = calcArcCoverage(ARCHIVE,NUM_USERS)\n",
    "            COVERAGE_ARR.append(cov)\n",
    "            \n",
    "            newTrialArchiveTxt(os.path.join(p,f))\n",
    "        \n",
    "        #export report\n",
    "        cov_d=\"sp-exp_coverage-out/\"\n",
    "        if not os.path.exists(cov_d):\n",
    "            os.mkdir(cov_d)\n",
    "        cov_report_filename = f\"[SPEXP-coverage]_report_EXP-{EXP_NUM}.txt\"\n",
    "        archiveCoverageReport(COVERAGE_ARR,os.path.join(cov_d,cov_report_filename))\n",
    "        \n",
    "        \n",
    "        \n",
    "    # -- NOVELTY EXPERIMENT -- #\n",
    "    elif EXP_NUM == 3:\n",
    "        #intialize coverage array\n",
    "        COVERAGE_ARR = []\n",
    "        \n",
    "        for t in range(TRIALS):\n",
    "            print(f\"---------     TRIAL #{t+1}    ----------\")\n",
    "            \n",
    "            ARCHIVE = {}  #initialize empty archive\n",
    "\n",
    "            #set number of users and initial userset\n",
    "            if DATASET == \"CAPACITIVE\":\n",
    "                NUM_USERS = 720\n",
    "            else:\n",
    "                NUM_USERS = 720    #change to something else later\n",
    "            CURRENT_USERS = list(range(NUM_USERS))    \n",
    "\n",
    "            for AI in range(ARC_ITER):\n",
    "                print(f\"*** Archive iteration {AI+1} / {ARC_ITER} -- USERS: [{len(CURRENT_USERS)} / {NUM_USERS}] ***\")\n",
    "                #run cma-es\n",
    "                es = cma.CMAEvolutionStrategy(np.random.normal(0,1,100).tolist(), 1, {'maxiter':CMA_ITER})\n",
    "                es.optimize(fitness)\n",
    "                z = es.result[0]\n",
    "\n",
    "                #get user list and binary number\n",
    "                fmr_val = FMR_DESC[DATASET][FMR]\n",
    "                print_name = f\"print_{DATASET}_{fmr_val}-exp_{EXP_NUM}\"\n",
    "                gen_print = generateSample(z)\n",
    "                usersFound = getUserPredictions(gen_print,CLASSIFIER,DATASET,fmr_val,TT_SET,print_name)\n",
    "\n",
    "                #SUBPRINTS - REMOVE USERS FOUND\n",
    "                for u in usersFound:\n",
    "                    if u in CURRENT_USERS:\n",
    "                        CURRENT_USERS.remove(u)\n",
    "\n",
    "                #add to lv and classification bin string to archive\n",
    "                user_binstr = userlist2Bin(usersFound,NUM_USERS)\n",
    "                ARCHIVE[user_binstr] = z\n",
    "                \n",
    "                #add user list to novelty archive\n",
    "                zv = np.zeros(NUM_USERS)\n",
    "                for u in usersFound:\n",
    "                    zv[u] = 1\n",
    "                NOVELTY_ARCHIVE.append(zv)\n",
    "\n",
    "                #export entry to file\n",
    "                p,f = makeDefaultFilename(\"\")\n",
    "                addArchiveTxt(ARCHIVE,os.path.join(p,f))\n",
    "\n",
    "            #save coverage (single item in archive)\n",
    "            cov = calcArcCoverage(ARCHIVE,NUM_USERS)\n",
    "            COVERAGE_ARR.append(cov)\n",
    "            \n",
    "            newTrialArchiveTxt(os.path.join(p,f))\n",
    "        \n",
    "        #export report\n",
    "        cov_d=\"sp-exp_coverage-out/\"\n",
    "        if not os.path.exists(cov_d):\n",
    "            os.mkdir(cov_d)\n",
    "        cov_report_filename = f\"[SPEXP-coverage]_report_EXP-{EXP_NUM}.txt\"\n",
    "        archiveCoverageReport(COVERAGE_ARR,os.path.join(cov_d,cov_report_filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment denotion\n",
    "EXPs = [1,2,3]\n",
    "EXP_NUM = 1     #denotes which experiment is on [1,2,3]\n",
    "\n",
    "#Datasets\n",
    "DATASETs = [\"CAPACITIVE\"]\n",
    "DATASET=\"CAPACITIVE\"\n",
    "\n",
    "#FMR denotation\n",
    "FMRs = [1.0, 0.1, 0.01]\n",
    "FMR = 1.0    #denotes which FMR using\n",
    "\n",
    "#Train/Test\n",
    "TTs=[\"train\"]\n",
    "TT_SET=\"train\"   #denotes which set evaluating\n",
    "\n",
    "#Classifier\n",
    "CLASSs=[\"MLC\"]\n",
    "CLASSIFIER=\"MLC\"\n",
    "\n",
    "#do the thing\n",
    "for c in CLASSs:\n",
    "    for d in DATASETs:\n",
    "        #set dataset\n",
    "        X_train, y_clean, print_dict = importDataset()\n",
    "         \n",
    "        #set classifier (if needed)\n",
    "        if CLASSIFIER == \"MLC\":\n",
    "            MLC_MODEL = setMLC(DATASET)\n",
    "        \n",
    "        for t in TTs:\n",
    "            for f in FMRs:\n",
    "                for e in EXPs:\n",
    "                    \n",
    "                    #set parameters\n",
    "                    EXP_NUM = e\n",
    "                    FMR = f\n",
    "                    TT_SET = t\n",
    "                    DATASET = d\n",
    "                    CLASSIFIER = c\n",
    "                    \n",
    "                    print(f\"-- EXPERIMENT: {EXP_NUM}-- \")\n",
    "                    print(f\"FMR: {FMR}\")\n",
    "                    print(f\"T SET: {TT_SET}\")\n",
    "                    print(f\"DATASET: {DATASET}\")\n",
    "                    print(f\"CLASSIFIER: {CLASSIFIER}\")\n",
    "                    print(\"\")\n",
    "                    \n",
    "                    \n",
    "                    #set generator\n",
    "                    GENERATOR = import_VAE(DATASET,32)\n",
    "                    \n",
    "                    #run it!\n",
    "                    runExp()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
